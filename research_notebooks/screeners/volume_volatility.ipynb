{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Market Volume and Volatility Screener\n",
    "# This notebook analyzes cryptocurrency markets to identify high-volume and high-volatility trading opportunities\n",
    "# It uses CLOB (Central Limit Order Book) data to fetch and analyze candle data from various trading pairs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from core.data_sources.clob import CLOBDataSource\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from core.features.candles.volatility import Volatility, VolatilityConfig\n",
    "from core.features.candles.volume import Volume, VolumeConfig\n",
    "\n",
    "\n",
    "# Initialize CLOB data source for fetching market data\n",
    "clob = CLOBDataSource()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Configuration Parameters\n",
    "# These constants control the data fetching and analysis behavior\n",
    "\n",
    "# Market configuration\n",
    "CONNECTOR_NAME = \"binance_perpetual\"  # Exchange connector to use\n",
    "QUOTE_ASSET = \"USDT\"                  # Quote currency for trading pairs\n",
    "INTERVAL = \"1m\"                       # Candle interval (1 minute)\n",
    "\n",
    "# Technical analysis parameters\n",
    "VOLATILITY_WINDOW = 20        # Window size for volatility calculations (20 periods)\n",
    "VOLUME_SHORT_WINDOW = 60      # Short-term volume moving average window\n",
    "VOLUME_LONG_WINDOW = 600       # Long-term volume moving average window\n",
    "\n",
    "# Data fetching configuration\n",
    "FETCH_CANDLES = False         # Set to True to fetch fresh data from exchange\n",
    "DAYS = 15                    # Number of days of historical data to fetch\n",
    "BATCH_CANDLES_REQUEST = 2    # Number of concurrent requests when fetching\n",
    "SLEEP_REQUEST = 2.0          # Sleep time between batch requests (seconds)\n",
    "\n",
    "# Filtering parameters (used later in the notebook)\n",
    "VOLUME_QUANTILE = 0.5          # Markets must be above 50th percentile in volume\n",
    "NATR_QUANTILE = 0.5            # Markets must be above 50th percentile in volatility\n",
    "TOP_X_MARKETS = 20             # Number of top markets to analyze"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Fetching and Caching\n",
    "This section either fetches fresh data from the exchange or loads cached data. Caching helps avoid unnecessary API calls and speeds up repeated analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch or load candle data\n",
    "# If FETCH_CANDLES is True: Downloads fresh data from the exchange\n",
    "# If FETCH_CANDLES is False: Loads previously cached data\n",
    "\n",
    "if FETCH_CANDLES:\n",
    "    # Get all available trading rules from the exchange\n",
    "    trading_rules = await clob.get_trading_rules(CONNECTOR_NAME)\n",
    "    \n",
    "    # Filter trading pairs to exclude those with matching quote asset\n",
    "    # This ensures we only get USDT-quoted pairs (e.g., BTC-USDT, ETH-USDT)\n",
    "    trading_pairs = [\n",
    "        trading_pair for trading_pair in trading_rules.get_all_trading_pairs() \n",
    "        if trading_pair.split(\"-\")[1] != QUOTE_ASSET\n",
    "    ]\n",
    "    \n",
    "    # Fetch candle data for all trading pairs\n",
    "    # Uses batch processing to avoid rate limiting\n",
    "    candles = await clob.get_candles_batch_last_days(\n",
    "        CONNECTOR_NAME, \n",
    "        trading_pairs, \n",
    "        INTERVAL, \n",
    "        DAYS, \n",
    "        BATCH_CANDLES_REQUEST, \n",
    "        SLEEP_REQUEST\n",
    "    )\n",
    "    \n",
    "    # Save fetched data to cache for future use\n",
    "    clob.dump_candles_cache()\n",
    "    print(f\"Fetched data for {len(candles)} trading pairs\")\n",
    "else:\n",
    "    # Load previously cached data\n",
    "    clob.load_candles_cache()\n",
    "    print(\"Loaded data from cache\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter candles to only include the desired connector and interval\n",
    "# The cache may contain data from multiple connectors and intervals\n",
    "candles = [\n",
    "    value for key, value in clob.candles_cache.items() \n",
    "    if key[2] == INTERVAL and key[0] == CONNECTOR_NAME\n",
    "]\n",
    "print(f\"Found {len(candles)} candle datasets for {CONNECTOR_NAME} with {INTERVAL} interval\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "# Market Analysis: Calculate Volume and Volatility Metrics with Improved Momentum Analysis\n# This uses the simplified volume feature that provides meaningful buy/sell momentum signals\n\n# Initialize configurations for technical indicators\nvolatility_config = VolatilityConfig(window=VOLATILITY_WINDOW)\nvolume_config = VolumeConfig(short_window=VOLUME_SHORT_WINDOW, long_window=VOLUME_LONG_WINDOW)\n\n# Process each trading pair and calculate metrics\nreport = []\nfor candle in candles:\n    try:\n        # Add volatility and volume features to the candle data\n        candle.add_features([\n            Volatility(volatility_config), \n            Volume(volume_config)\n        ])\n        df = candle.data\n        \n        # Calculate volatility metrics\n        mean_volatility = df['volatility'].mean()\n        mean_natr = df['natr'].mean()\n        mean_bb_width = df['bb_width'].mean()\n        \n        # Get meaningful trend signals from improved volume metrics\n        latest_divergence = df['buy_pressure_divergence'].iloc[-1]\n        latest_momentum = df['buy_momentum'].iloc[-1]\n        latest_volume_surge = df['volume_surge'].iloc[-1]\n        latest_buy_signal = df['volume_buy_signal'].iloc[-1]\n        latest_accumulation = df['accumulation_score'].iloc[-1]\n        latest_distribution = df['distribution_score'].iloc[-1]\n        \n        # Calculate volume metrics\n        total_volume_usd = df['volume_usd'].sum()\n        total_hours = (df.index[-1] - df.index[0]).total_seconds() / 3600\n        average_volume_per_hour = total_volume_usd / total_hours\n        \n        # Price range analysis\n        max_price = df['close'].max()\n        min_price = df['close'].min()\n        range_price = max_price - min_price\n        \n        # Calculate how far current price is from the high (as percentage)\n        range_price_pct = (max_price - df['close'].iloc[-1]) / df['close'].iloc[-1]\n        \n        # Current position in the range (0 = at min, 1 = at max)\n        current_position = (df['close'].iloc[-1] - min_price) / range_price if range_price > 0 else 0.5\n        \n        # Enhanced composite score incorporating momentum signals\n        # Normalize divergence to 0-1 scale (0.5 is neutral)\n        normalized_divergence = (latest_divergence + 1) / 2 if not pd.isna(latest_divergence) else 0.5\n        \n        # Calculate momentum-weighted score\n        momentum_factor = max(0.1, normalized_divergence * latest_volume_surge)\n        \n        score = (\n            mean_natr * \n            average_volume_per_hour * \n            momentum_factor * \n            (1 - abs(current_position - 0.5))  # Prefer mid-range positions\n        )\n        \n        report.append({\n            'trading_pair': candle.trading_pair,\n            'mean_volatility': mean_volatility,\n            'mean_natr': mean_natr,\n            'mean_bb_width': mean_bb_width,\n            'buy_pressure_divergence': latest_divergence,\n            'buy_momentum': latest_momentum,\n            'volume_surge': latest_volume_surge,\n            'volume_buy_signal': latest_buy_signal,\n            'accumulation_score': latest_accumulation,\n            'distribution_score': latest_distribution,\n            'average_volume_per_hour': average_volume_per_hour,\n            'current_position': current_position,\n            'range_price_pct': range_price_pct,\n            'score': score\n        })\n    except Exception as e:\n        print(f\"Error processing {candle.trading_pair}: {e}\")\n        continue\n\n# Convert to DataFrame and normalize scores\nreport_df = pd.DataFrame(report)\nmax_score = report_df['score'].max()\nreport_df['normalized_score'] = report_df['score'] / max_score if max_score > 0 else 0\nreport_df.drop(columns=['score'], inplace=True)\n\n# Display the report sorted by normalized score\nprint(f\"\\n📊 MARKET ANALYSIS COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"Analyzed {len(report_df)} trading pairs\")\nprint(\"\\n🎯 TOP 10 MARKETS BY MOMENTUM-WEIGHTED SCORE:\")\nprint(\"-\" * 60)\n\n# Show top markets with key metrics\ntop_10 = report_df.sort_values('normalized_score', ascending=False).head(10)\ndisplay_cols = ['trading_pair', 'normalized_score', 'buy_pressure_divergence', \n                'volume_surge', 'accumulation_score', 'mean_natr', 'average_volume_per_hour']\ntop_10[display_cols]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Momentum Analysis: Understanding Buy/Sell Pressure Dynamics\nprint(\"\\n🔍 MOMENTUM INSIGHTS\")\nprint(\"=\" * 60)\n\n# Identify markets with strong momentum signals\nbullish_divergence = report_df[report_df['buy_pressure_divergence'] > 0.05].sort_values('buy_pressure_divergence', ascending=False)\nbearish_divergence = report_df[report_df['buy_pressure_divergence'] < -0.05].sort_values('buy_pressure_divergence')\n\nprint(f\"\\n📈 BULLISH MOMENTUM (Short-term buying > Long-term):\")\nprint(\"-\" * 40)\nif not bullish_divergence.empty:\n    for _, row in bullish_divergence.head(5).iterrows():\n        print(f\"  {row['trading_pair']:10s} | Divergence: {row['buy_pressure_divergence']:+.4f} | Volume Surge: {row['volume_surge']:.2f}x\")\nelse:\n    print(\"  No markets showing strong bullish divergence\")\n\nprint(f\"\\n📉 BEARISH MOMENTUM (Short-term selling > Long-term):\")\nprint(\"-\" * 40)\nif not bearish_divergence.empty:\n    for _, row in bearish_divergence.head(5).iterrows():\n        print(f\"  {row['trading_pair']:10s} | Divergence: {row['buy_pressure_divergence']:+.4f} | Volume Surge: {row['volume_surge']:.2f}x\")\nelse:\n    print(\"  No markets showing strong bearish divergence\")\n\n# Identify accumulation and distribution zones\naccumulation_markets = report_df[report_df['accumulation_score'] > report_df['accumulation_score'].quantile(0.8)]\ndistribution_markets = report_df[report_df['distribution_score'] > report_df['distribution_score'].quantile(0.8)]\n\nprint(f\"\\n💰 ACCUMULATION ZONES (Smart money buying):\")\nprint(\"-\" * 40)\nif not accumulation_markets.empty:\n    for _, row in accumulation_markets.head(5).iterrows():\n        print(f\"  {row['trading_pair']:10s} | Score: {row['accumulation_score']:.4f} | Volume: ${row['average_volume_per_hour']:,.0f}/hr\")\nelse:\n    print(\"  No clear accumulation patterns detected\")\n\nprint(f\"\\n🔴 DISTRIBUTION ZONES (Smart money selling):\")\nprint(\"-\" * 40)\nif not distribution_markets.empty:\n    for _, row in distribution_markets.head(5).iterrows():\n        print(f\"  {row['trading_pair']:10s} | Score: {row['distribution_score']:.4f} | Volume: ${row['average_volume_per_hour']:,.0f}/hr\")\nelse:\n    print(\"  No clear distribution patterns detected\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Market Filtering Strategy\n",
    "# Filter markets based on quantile thresholds to identify the best trading opportunities\n",
    "# Markets must meet minimum volume and volatility requirements and have favorable price positioning\n",
    "\n",
    "# Filtering parameters (adjust these to be more or less selective)\n",
    "VOLUME_QUANTILE = 0.5          # Markets must be above 50th percentile in volume\n",
    "NATR_QUANTILE = 0.5            # Markets must be above 50th percentile in volatility\n",
    "TOP_X_MARKETS = 20             # Number of top markets to analyze\n",
    "\n",
    "# Apply filters to identify high-opportunity markets\n",
    "# These are markets with good liquidity, high volatility, and favorable price positioning\n",
    "top_markets = report_df[\n",
    "    (report_df['average_volume_per_hour'] > report_df['average_volume_per_hour'].quantile(VOLUME_QUANTILE)) &\n",
    "    (report_df['mean_natr'] > report_df['mean_natr'].quantile(NATR_QUANTILE))\n",
    "]\n",
    "\n",
    "# Sort by current position (markets closest to their highs) and take top X\n",
    "top_markets = top_markets.sort_values(by='current_position', ascending=False).head(TOP_X_MARKETS)\n",
    "\n",
    "# Create a dictionary of candles for selected markets for detailed analysis\n",
    "top_markets_candles = {\n",
    "    candle.trading_pair: candle \n",
    "    for candle in candles \n",
    "    if candle.trading_pair in top_markets[\"trading_pair\"].values\n",
    "}\n",
    "\n",
    "print(f\"\\\\n🎯 FILTERED MARKETS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Applied filters:\")\n",
    "print(f\"  - Volume > {VOLUME_QUANTILE*100}th percentile\")\n",
    "print(f\"  - Volatility (NATR) > {NATR_QUANTILE*100}th percentile\")\n",
    "print(f\"\\\\nMarkets meeting criteria: {len(top_markets_candles)}\")\n",
    "print(f\"Selected markets: {list(top_markets_candles.keys())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Enhanced Correlation Analysis: Volume, Volatility, and Momentum Metrics\n# This helps understand relationships between volume, volatility, and buy/sell momentum\n\n# Select key metrics for correlation analysis\ncorrelation_data = report_df[[\n    'average_volume_per_hour', 'mean_volatility', 'mean_natr', \n    'buy_pressure_divergence', 'volume_surge', 'volume_buy_signal',\n    'accumulation_score', 'distribution_score', 'current_position'\n]].copy()\n\n# Log transform volume for better correlation analysis\ncorrelation_data['log_volume'] = np.log10(correlation_data['average_volume_per_hour'] + 1)\n\n# Calculate correlation matrix\ncorr_matrix = correlation_data.corr()\n\n# Create enhanced correlation heatmap with Plotly\nfig_corr = go.Figure(data=go.Heatmap(\n    z=corr_matrix.values,\n    x=corr_matrix.columns,\n    y=corr_matrix.columns,\n    colorscale='RdBu',\n    zmid=0,\n    text=np.round(corr_matrix.values, 2),\n    texttemplate='%{text}',\n    textfont={\"size\": 10},\n    colorbar=dict(title=\"Correlation\")\n))\n\nfig_corr.update_layout(\n    title=\"Enhanced Correlation Matrix: Volume, Volatility & Momentum Metrics\",\n    height=700,\n    width=900,\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n    xaxis={'tickangle': 45}\n)\n\nfig_corr.show()\n\n# Key correlation insights\nprint(\"\\n🔍 CORRELATION INSIGHTS\")\nprint(\"=\" * 60)\n\n# Volume-Momentum correlations\nvol_divergence_corr = correlation_data[['log_volume', 'buy_pressure_divergence']].corr().iloc[0, 1]\nvol_surge_corr = correlation_data[['log_volume', 'volume_surge']].corr().iloc[0, 1]\n\nprint(f\"\\n📊 Volume-Momentum Relationships:\")\nprint(f\"  - Volume vs Buy Pressure Divergence: {vol_divergence_corr:.3f}\")\nif abs(vol_divergence_corr) > 0.3:\n    print(f\"    → {'High volume markets show stronger momentum shifts' if vol_divergence_corr > 0 else 'High volume markets show momentum stability'}\")\n\nprint(f\"  - Volume vs Volume Surge: {vol_surge_corr:.3f}\")\nprint(f\"    → {'Large markets tend to have volume spikes' if vol_surge_corr > 0 else 'Smaller markets show more volume volatility'}\")\n\n# Accumulation vs Distribution correlation\nacc_dist_corr = correlation_data[['accumulation_score', 'distribution_score']].corr().iloc[0, 1]\nprint(f\"\\n💰 Market Regime Analysis:\")\nprint(f\"  - Accumulation vs Distribution: {acc_dist_corr:.3f}\")\nif acc_dist_corr < -0.5:\n    print(\"    → Strong inverse relationship: Markets are clearly in either accumulation OR distribution\")\nelse:\n    print(\"    → Mixed regimes: Some markets show both accumulation and distribution characteristics\")\n\n# Momentum-Volatility relationship\nmomentum_vol_corr = correlation_data[['buy_pressure_divergence', 'mean_natr']].corr().iloc[0, 1]\nprint(f\"\\n🎢 Momentum-Volatility Relationship:\")\nprint(f\"  - Buy Pressure Divergence vs NATR: {momentum_vol_corr:.3f}\")\nif abs(momentum_vol_corr) > 0.3:\n    print(f\"    → {'Volatile markets show stronger momentum shifts' if momentum_vol_corr > 0 else 'Stable markets show clearer momentum patterns'}\")\n\n# Statistical tests for key relationships\nfrom scipy import stats\n\n# Test if volume surge predicts buy pressure divergence\nif len(correlation_data) > 10:\n    slope, intercept, r_value, p_value, std_err = stats.linregress(\n        correlation_data['volume_surge'].fillna(1), \n        correlation_data['buy_pressure_divergence'].fillna(0)\n    )\n    print(f\"\\n📈 PREDICTIVE ANALYSIS (Volume Surge → Buy Pressure Divergence)\")\n    print(f\"  - R-squared: {r_value**2:.3f}\")\n    print(f\"  - P-value: {p_value:.4f}\")\n    if p_value < 0.05:\n        print(f\"  → Volume surges {'predict bullish momentum' if slope > 0 else 'predict bearish momentum'} (statistically significant!)\")\n    else:\n        print(f\"  → No significant predictive relationship found\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
